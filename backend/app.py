from flask import Flask, request, jsonify, send_file, Response, session
from flask_cors import CORS
import os
import re
import json
import ast
import io
import requests
from datetime import datetime, timedelta
from dotenv import load_dotenv
import base64
import hashlib
import threading
import time
from collections import OrderedDict

# ÃncÄƒrcare variabile din .env
load_dotenv()

app = Flask(__name__)
CORS(app)

# Configurare sesiune
app.config['SECRET_KEY'] = os.getenv('SECRET_KEY', 'dev-secret-key')
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(hours=int(os.getenv('SESSION_TIMEOUT', 3600)) / 3600)

# FAZA 2.1 - Limite pentru prevenirea memory leak
MAX_SESSION_EDITS = int(os.getenv('MAX_SESSION_FILES', 100))
MAX_FILE_SIZE = int(os.getenv('MAX_FILE_SIZE', 10485760))  # 10MB
SESSION_CLEANUP_INTERVAL = 3600  # 1 orÄƒ

# FAZA 2.1 - Cache pentru fiÈ™iere editate cu limitÄƒ È™i LRU
class LimitedSessionCache:
    """Cache cu limitÄƒ pentru editÄƒrile din sesiune"""
    def __init__(self, max_size=MAX_SESSION_EDITS):
        self.cache = OrderedDict()
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get(self, key):
        with self.lock:
            if key in self.cache:
                # MutÄƒ la final (most recently used)
                self.cache.move_to_end(key)
                return self.cache[key]
            return None
    
    def set(self, key, value):
        with self.lock:
            if key in self.cache:
                # ActualizeazÄƒ È™i mutÄƒ la final
                self.cache.move_to_end(key)
            self.cache[key] = value
            
            # EliminÄƒ cele mai vechi dacÄƒ depÄƒÈ™im limita
            while len(self.cache) > self.max_size:
                self.cache.popitem(last=False)
    
    def delete(self, key):
        with self.lock:
            if key in self.cache:
                del self.cache[key]
    
    def clear(self):
        with self.lock:
            self.cache.clear()
    
    def size(self):
        with self.lock:
            return len(self.cache)
    
    def items(self):
        with self.lock:
            return list(self.cache.items())

# ÃnlocuieÈ™te dicÈ›ionarul simplu cu cache-ul limitat
session_edits = LimitedSessionCache()

# Cache pentru structura de directoare
directory_structures = {}
directory_files = {}

# FAZA 2.1 - Thread pentru curÄƒÈ›are periodicÄƒ
def cleanup_old_sessions():
    """CurÄƒÈ›Äƒ periodic sesiunile vechi"""
    while True:
        time.sleep(SESSION_CLEANUP_INTERVAL)
        try:
            # CurÄƒÈ›Äƒ structurile de directoare mai vechi de 24 ore
            current_time = datetime.now()
            structures_to_remove = []
            
            for struct_id in list(directory_structures.keys()):
                # VerificÄƒ dacÄƒ existÄƒ timestamp
                if hasattr(directory_structures[struct_id], '_timestamp'):
                    if current_time - directory_structures[struct_id]._timestamp > timedelta(hours=24):
                        structures_to_remove.append(struct_id)
                        
            for struct_id in structures_to_remove:
                del directory_structures[struct_id]
                if struct_id in directory_files:
                    del directory_files[struct_id]
                    
            if structures_to_remove:
                print(f"CurÄƒÈ›at {len(structures_to_remove)} structuri vechi")
                
        except Exception as e:
            print(f"Eroare la curÄƒÈ›area sesiunilor: {e}")

# PorneÈ™te thread-ul de curÄƒÈ›are
cleanup_thread = threading.Thread(target=cleanup_old_sessions, daemon=True)
cleanup_thread.start()

# Import analizoare actualizate
from analyzers.ast_analyzer import ASTAnalyzer
from analyzers.dependency_analyzer import DependencyAnalyzer
from analyzers.project_analyzer import ProjectAnalyzer

# IniÈ›ializare analizoare
ast_analyzer = ASTAnalyzer()
dependency_analyzer = DependencyAnalyzer()
project_analyzer = ProjectAnalyzer()

def calculate_complexity(node):
    """CalculeazÄƒ complexitatea ciclomaticÄƒ a unei funcÈ›ii"""
    complexity = 1
    for child in ast.walk(node):
        if isinstance(child, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
            complexity += 1
        elif isinstance(child, ast.BoolOp):
            complexity += len(child.values) - 1
    return complexity

def extract_entities_from_code(code):
    """Extrage funcÈ›iile È™i clasele dintr-un cod Python"""
    entities = {'functions': [], 'classes': []}
    try:
        tree = ast.parse(code)
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                entities['functions'].append(node.name)
            elif isinstance(node, ast.ClassDef):
                entities['classes'].append(node.name)
    except:
        # Fallback la regex dacÄƒ AST parsing eÈ™ueazÄƒ
        lines = code.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('def '):
                match = re.match(r'def\s+(\w+)', line)
                if match:
                    entities['functions'].append(match.group(1))
            elif line.startswith('class '):
                match = re.match(r'class\s+(\w+)', line)
                if match:
                    entities['classes'].append(match.group(1))
    return entities

def analyze_imports_detailed(content, modul_principal, entities):
    """FAZA 1.3 - AnalizeazÄƒ detaliat ce entitÄƒÈ›i sunt importate, folosind AST analyzer actualizat"""
    imported_entities = {'functions': [], 'classes': []}
    
    # FoloseÈ™te AST analyzer pentru detectare precisÄƒ
    analysis = ast_analyzer.analyze_code(content)
    imports_detail = analysis.get('imports_detail', {})
    
    # VerificÄƒ importurile pentru modulul principal
    if modul_principal in imports_detail:
        import_info = imports_detail[modul_principal]
        items = import_info.get('items', [])
        
        if items == ['*']:
            # Import complet - toate entitÄƒÈ›ile sunt disponibile
            return entities
        else:
            # Import specific
            for item in items:
                if item in entities['functions']:
                    imported_entities['functions'].append(item)
                elif item in entities['classes']:
                    imported_entities['classes'].append(item)
    
    # VerificÄƒ È™i importurile de tip "import modul"
    for module, info in imports_detail.items():
        if module == modul_principal and info['type'] == 'import':
            # Pentru "import modul", toate entitÄƒÈ›ile sunt accesibile via modul.entitate
            return entities
    
    return imported_entities

def find_entry_points(structure):
    """GÄƒseÈ™te punctele de intrare Ã®ntr-o structurÄƒ de directoare"""
    entry_points = []
    
    def search_files(node, path=''):
        current_path = f"{path}/{node['name']}" if path else node['name']
        
        # VerificÄƒ fiÈ™ierele din nodul curent
        for file in node.get('files', []):
            if file['name'] in ['main.py', 'app.py', '__main__.py', 'run.py', 'manage.py', 'cli.py']:
                entry_points.append({
                    'name': file['name'],
                    'path': f"{current_path}/{file['name']}",
                    'type': 'main'
                })
            elif file['name'] == 'setup.py':
                entry_points.append({
                    'name': file['name'],
                    'path': f"{current_path}/{file['name']}",
                    'type': 'setup'
                })
        
        # Recursiv pentru subdirectoare
        for child_name, child_node in node.get('children', {}).items():
            search_files(child_node, current_path)
    
    search_files(structure)
    return entry_points

def count_directories(structure):
    """NumÄƒrÄƒ directoarele dintr-o structurÄƒ"""
    count = 0
    
    def count_recursive(node):
        nonlocal count
        count += len(node.get('children', {}))
        for child in node.get('children', {}).values():
            count_recursive(child)
    
    count_recursive(structure)
    return count

def get_edited_content(filename):
    """ObÈ›ine conÈ›inutul editat dintr-un fiÈ™ier din sesiune"""
    return session_edits.get(filename)

def analyze_directory_dependencies(files):
    """AnalizeazÄƒ dependenÈ›ele Ã®ntre fiÈ™ierele unui director"""
    dependencies = {}
    
    for file in files:
        filename = file.get('name', '')
        if not filename.endswith('.py'):
            continue
            
        content = get_edited_content(filename) or file.get('content', '')
        if not content:
            continue
        
        analysis = ast_analyzer.analyze_code(content, filename)
        deps = set()
        
        # VerificÄƒ importurile locale
        imports_detail = analysis.get('imports_detail', {})
        for imp_module in imports_detail:
            # VerificÄƒ dacÄƒ este un import local
            for other_file in files:
                other_name = other_file.get('name', '').replace('.py', '')
                if imp_module == other_name or imp_module.endswith(f'.{other_name}'):
                    deps.add(other_file.get('name', ''))
        
        dependencies[filename] = list(deps)
    
    return dependencies

def calculate_analysis_times(project_data):
    """CalculeazÄƒ timpii estimaÈ›i pentru analizÄƒ"""
    num_files = len(project_data.get('secondary_scripts', [])) + 1
    num_connections = len(project_data.get('connections', []))
    
    times = {
        'script_analysis': round(num_files * 0.1, 2),
        'dependency_analysis': round(num_connections * 0.05, 2),
        'pattern_analysis': round(num_files * 0.15, 2),
        'report_generation': round(0.5, 2)
    }
    
    times['total'] = round(sum(times.values()), 2)
    return times

def identify_bottlenecks(project_data):
    """IdentificÄƒ potenÈ›ialele probleme Ã®n structura proiectului"""
    bottlenecks = []
    
    # VerificÄƒ dependenÈ›e circulare
    connections = project_data.get('connections', [])
    if len(connections) > len(project_data.get('secondary_scripts', [])) * 2:
        bottlenecks.append({
            'type': 'CRITICAL',
            'message': 'Posibile dependenÈ›e circulare detectate',
            'severity': 'high'
        })
    
    # VerificÄƒ complexitate
    main_script = project_data.get('main_script', {})
    if main_script.get('analysis', {}).get('functions', []):
        total_complexity = sum(f.get('complexity', 1) for f in main_script['analysis']['functions'])
        if total_complexity > 50:
            bottlenecks.append({
                'type': 'WARNING',
                'message': f'Complexitate ridicatÄƒ Ã®n scriptul principal: {total_complexity}',
                'severity': 'medium'
            })
    
    return bottlenecks

# ========== ENDPOINTS API ==========

@app.route('/')
def home():
    return 'Python Forensics (p4n6) - PlatformÄƒ avansatÄƒ de analizÄƒ pentru proiecte Python'

@app.route('/set_principal', methods=['POST'])
def set_principal():
    """SeteazÄƒ scriptul principal pentru analizÄƒ"""
    try:
        data = request.get_json()
        content = data.get('content', '')
        filename = data.get('filename', 'principal') + '.py'
        
        # FAZA 2.1 - VerificÄƒ dimensiunea
        if len(content.encode('utf-8')) > MAX_FILE_SIZE:
            return jsonify({
                'status': 'error',
                'message': f'FiÈ™ierul depÄƒÈ™eÈ™te limita de {MAX_FILE_SIZE // 1024 // 1024}MB'
            }), 413
        
        # SalveazÄƒ Ã®n cache sesiune cu limitÄƒ
        session_edits.set(filename, content)
        
        # AnalizÄƒ folosind AST analyzer actualizat
        analysis = ast_analyzer.analyze_code(content, filename)
        
        # Extrage entitÄƒÈ›ile pentru compatibilitate
        entities = {
            'functions': [f.name for f in analysis.get('functions', [])],
            'classes': [c.name for c in analysis.get('classes', [])]
        }
        
        # SalveazÄƒ pentru compatibilitate cu analiza existentÄƒ
        base_dir = os.path.dirname(__file__)
        principal_path = os.path.join(base_dir, 'principal.txt')
        entities_path = os.path.join(base_dir, 'entities.json')
        
        with open(principal_path, 'w', encoding='utf-8') as f:
            f.write(data.get('filename', 'principal'))
        
        with open(entities_path, 'w', encoding='utf-8') as f:
            json.dump(entities, f, ensure_ascii=False, indent=2)
        
        return jsonify({
            'status': 'ok',
            'entities': entities,
            'analysis': analysis
        })
        
    except Exception as e:
        # FAZA 3.2 - Gestionare erori user-friendly
        return jsonify({
            'status': 'error',
            'message': f'Eroare la setarea scriptului principal: {str(e)}'
        }), 500

@app.route('/add_secundar', methods=['POST'])
def add_secundar():
    """AdaugÄƒ un script secundar pentru analizÄƒ"""
    try:
        data = request.get_json()
        filename = data.get('filename', '')
        content = data.get('content', '')
        
        if not filename:
            return jsonify({'status': 'error', 'message': 'Numele fiÈ™ierului este necesar'}), 400
        
        # FAZA 2.1 - VerificÄƒ dimensiunea
        if len(content.encode('utf-8')) > MAX_FILE_SIZE:
            return jsonify({
                'status': 'error',
                'message': f'FiÈ™ierul depÄƒÈ™eÈ™te limita de {MAX_FILE_SIZE // 1024 // 1024}MB'
            }), 413
        
        # SalveazÄƒ Ã®n cache sesiune cu limitÄƒ
        session_edits.set(filename, content)
        
        # Pentru compatibilitate cu sistemul existent
        base_dir = os.path.dirname(__file__)
        parent_dir = os.path.abspath(os.path.join(base_dir, ".."))
        secundare_path = os.path.join(parent_dir, 'secundare.txt')
        
        with open(secundare_path, 'a', encoding='utf-8') as f:
            f.write(f"{filename}\n")
        
        return jsonify({
            'status': 'ok',
            'message': 'Script secundar adÄƒugat cu succes',
            'cache_size': session_edits.size()
        })
        
    except Exception as e:
        # FAZA 3.2 - Gestionare erori user-friendly
        return jsonify({
            'status': 'error',
            'message': f'Eroare la adÄƒugarea scriptului secundar: {str(e)}'
        }), 500

@app.route('/analyze_imports', methods=['POST'])
def analyze_imports():
    """AnalizeazÄƒ importurile dintr-un script secundar"""
    try:
        data = request.get_json()
        content = data.get('content', '')
        filename = data.get('filename', '')
        
        base_dir = os.path.dirname(__file__)
        principal_path = os.path.join(base_dir, 'principal.txt')
        entities_path = os.path.join(base_dir, 'entities.json')
        
        # CiteÈ™te modulul principal È™i entitÄƒÈ›ile
        modul_principal = ''
        if os.path.exists(principal_path):
            with open(principal_path, 'r', encoding='utf-8') as f:
                modul_principal = f.read().strip()
        
        entities = {'functions': [], 'classes': []}
        if os.path.exists(entities_path):
            with open(entities_path, 'r', encoding='utf-8') as f:
                entities = json.load(f)
        
        # FAZA 1.3 - FoloseÈ™te AST analyzer actualizat pentru analizÄƒ completÄƒ
        analysis = ast_analyzer.analyze_code(content, filename)
        imports_detail = analysis.get('imports_detail', {})
        
        # VerificÄƒ dacÄƒ importÄƒ modulul principal
        imports_module = modul_principal in imports_detail
        
        result = {
            'imports': imports_module,
            'entities': {'functions': [], 'classes': []},
            'analysis': analysis
        }
        
        if imports_module:
            # AnalizeazÄƒ detaliat ce importÄƒ
            result['entities'] = analyze_imports_detailed(content, modul_principal, entities)
        
        return jsonify(result)
        
    except Exception as e:
        # FAZA 3.2 - Gestionare erori user-friendly
        return jsonify({
            'status': 'error',
            'message': f'Eroare la analiza importurilor: {str(e)}',
            'imports': False,
            'entities': {'functions': [], 'classes': []},
            'analysis': {}
        }), 500

@app.route('/get_analysis', methods=['GET'])
def get_analysis():
    """ReturneazÄƒ analiza completÄƒ salvatÄƒ"""
    try:
        base_dir = os.path.dirname(__file__)
        analysis_path = os.path.join(base_dir, 'analysis.json')
        
        result = {
            'status': 'ok',
            'has_analysis': os.path.exists(analysis_path),
            'session_edits': session_edits.size(),
            'detailed_imports': {}
        }
        
        # ReturneazÄƒ analiza detaliatÄƒ dacÄƒ existÄƒ
        if os.path.exists(analysis_path):
            with open(analysis_path, 'r', encoding='utf-8') as f:
                result['detailed_imports'] = json.load(f)
        
        return jsonify(result)
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la obÈ›inerea analizei: {str(e)}'
        }), 500

@app.route('/save_directory_structure', methods=['POST'])
def save_directory_structure():
    """SalveazÄƒ structura de directoare pentru analizÄƒ ulterioarÄƒ"""
    try:
        data = request.get_json()
        structure = data.get('structure', {})
        files = data.get('files', [])
        
        # GenereazÄƒ un ID unic pentru structurÄƒ
        structure_id = hashlib.md5(json.dumps(structure, sort_keys=True).encode()).hexdigest()[:8]
        
        # FAZA 2.1 - AdaugÄƒ timestamp pentru curÄƒÈ›are ulterioarÄƒ
        structure._timestamp = datetime.now()
        
        # SalveazÄƒ Ã®n cache
        directory_structures[structure_id] = structure
        directory_files[structure_id] = files
        
        # AnalizÄƒ punctele de intrare
        entry_points = find_entry_points(structure)
        
        return jsonify({
            'status': 'ok',
            'structure_id': structure_id,
            'entry_points': entry_points,
            'total_files': len(files),
            'python_files': len([f for f in files if f.get('type') == 'python'])
        })
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la salvarea structurii: {str(e)}'
        }), 500

@app.route('/get_file_content', methods=['POST'])
def get_file_content():
    """ObÈ›ine conÈ›inutul unui fiÈ™ier din structura salvatÄƒ"""
    try:
        data = request.get_json()
        structure_id = data.get('structure_id', '')
        file_path = data.get('file_path', '')
        
        if structure_id not in directory_files:
            return jsonify({'status': 'error', 'message': 'StructurÄƒ necunoscutÄƒ'}), 404
        
        files = directory_files[structure_id]
        
        # CautÄƒ fiÈ™ierul dupÄƒ path
        for file_data in files:
            if file_data.get('path', '') == file_path or file_data.get('name', '') == file_path:
                # VerificÄƒ dacÄƒ existÄƒ o versiune editatÄƒ
                edited_content = get_edited_content(file_data.get('name', ''))
                if edited_content:
                    file_data['content'] = edited_content
                    
                return jsonify({
                    'status': 'ok',
                    'file': {
                        'name': file_data.get('name', ''),
                        'content': file_data.get('content', ''),
                        'type': file_data.get('type', 'unknown')
                    }
                })
        
        return jsonify({'status': 'error', 'message': 'FiÈ™ier negÄƒsit'}), 404
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la obÈ›inerea conÈ›inutului: {str(e)}'
        }), 500

@app.route('/analyze_directory', methods=['POST'])
def analyze_directory():
    """AnalizeazÄƒ complet o structurÄƒ de directoare"""
    try:
        data = request.get_json()
        structure_id = data.get('structure_id', '')
        
        if structure_id not in directory_structures:
            return jsonify({'status': 'error', 'message': 'StructurÄƒ necunoscutÄƒ'}), 404
        
        structure = directory_structures[structure_id]
        files = directory_files.get(structure_id, [])
        
        # FiltreazÄƒ doar fiÈ™ierele Python
        python_files = [f for f in files if f.get('type') == 'python' and f.get('content')]
        
        analysis_results = {
            'structure_id': structure_id,
            'timestamp': datetime.now().isoformat(),
            'files_analyzed': 0,
            'total_files': len(files),
            'total_functions': 0,
            'total_classes': 0,
            'total_lines': 0,
            'dependencies': {},
            'entry_points': find_entry_points(structure),
            'file_analyses': {},
            'import_graph': {},
            'complexity_metrics': {},
            'directories': count_directories(structure)
        }
        
        # AnalizeazÄƒ fiecare fiÈ™ier Python
        for file_data in python_files:
            filename = file_data['name']
            content = get_edited_content(filename) or file_data.get('content', '')
            
            if not content or content == '[FiÈ™ier prea mare - conÈ›inutul nu a fost Ã®ncÄƒrcat automat]':
                continue
            
            # AnalizÄƒ detaliatÄƒ folosind AST analyzer actualizat
            file_analysis = ast_analyzer.analyze_code(content, filename)
            analysis_results['file_analyses'][filename] = file_analysis
            
            # ActualizeazÄƒ statistici globale
            if 'error' not in file_analysis:
                analysis_results['files_analyzed'] += 1
                analysis_results['total_functions'] += len(file_analysis.get('functions', []))
                analysis_results['total_classes'] += len(file_analysis.get('classes', []))
                analysis_results['total_lines'] += file_analysis.get('metrics', {}).get('total_lines', 0)
                
                # Complexitate
                functions = file_analysis.get('functions', [])
                if functions:
                    total_complexity = sum(f.complexity for f in functions)
                    avg_complexity = total_complexity / len(functions)
                    max_complexity = max(f.complexity for f in functions)
                    
                    analysis_results['complexity_metrics'][filename] = {
                        'total': total_complexity,
                        'average': round(avg_complexity, 2),
                        'max': max_complexity
                    }
        
        # AnalizeazÄƒ dependenÈ›ele
        analysis_results['dependencies'] = analyze_directory_dependencies(python_files)
        
        # ConstruieÈ™te graful de importuri
        for filename, deps in analysis_results['dependencies'].items():
            analysis_results['import_graph'][filename] = {
                'imports': deps,
                'imported_by': []
            }
        
        # PopuleazÄƒ imported_by
        for filename, graph_data in analysis_results['import_graph'].items():
            for dep in graph_data['imports']:
                if dep in analysis_results['import_graph']:
                    analysis_results['import_graph'][dep]['imported_by'].append(filename)
        
        return jsonify({
            'status': 'ok',
            'analysis': analysis_results
        })
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la analiza directorului: {str(e)}'
        }), 500

@app.route('/save_session_edit', methods=['POST'])
def save_session_edit():
    """SalveazÄƒ o editare Ã®n sesiune"""
    try:
        data = request.get_json()
        filename = data.get('filename', '')
        content = data.get('content', '')
        
        if not filename:
            return jsonify({'status': 'error', 'message': 'Numele fiÈ™ierului este necesar'}), 400
        
        # FAZA 2.1 - VerificÄƒ dimensiunea
        if len(content.encode('utf-8')) > MAX_FILE_SIZE:
            return jsonify({
                'status': 'error',
                'message': f'FiÈ™ierul depÄƒÈ™eÈ™te limita de {MAX_FILE_SIZE // 1024 // 1024}MB'
            }), 413
        
        # SalveazÄƒ cu cache limitat
        session_edits.set(filename, content)
        
        return jsonify({
            'status': 'ok',
            'message': 'Editare salvatÄƒ Ã®n sesiune',
            'total_edits': session_edits.size()
        })
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la salvarea editÄƒrii: {str(e)}'
        }), 500

@app.route('/get_session_edits', methods=['GET'])
def get_session_edits():
    """ReturneazÄƒ toate editÄƒrile din sesiune"""
    try:
        edits_dict = dict(session_edits.items())
        return jsonify({
            'edits': edits_dict,
            'count': len(edits_dict)
        })
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la obÈ›inerea editÄƒrilor: {str(e)}',
            'edits': {},
            'count': 0
        }), 500

@app.route('/generate_workflow', methods=['POST'])
def generate_workflow():
    """GenereazÄƒ workflow profesional folosind Claude API"""
    try:
        data = request.get_json()
        
        # FAZA 3.2 - VerificÄƒ cheia API cu mesaj user-friendly
        api_key = os.getenv('ANTHROPIC_API_KEY')
        if not api_key or api_key == 'your-anthropic-api-key-here':
            return jsonify({
                'status': 'error',
                'message': 'Cheia API Anthropic nu este configuratÄƒ. VÄƒ rugÄƒm sÄƒ adÄƒugaÈ›i ANTHROPIC_API_KEY Ã®n fiÈ™ierul .env'
            }), 400
        
        # PregÄƒteÈ™te datele proiectului
        project_data = {
            "project_name": data.get('project_name', 'Analiza Proiect Python'),
            "analysis_timestamp": datetime.now().strftime("%Y-%m-%d %H:%M"),
            "main_script": data.get('main_script', {}),
            "secondary_scripts": data.get('secondary_scripts', []),
            "connections": data.get('connections', []),
            "metrics": data.get('metrics', {}),
            "directory_structure": data.get('directory_structure')
        }
        
        # CalculeazÄƒ timpii È™i identificÄƒ bottleneck-urile
        analysis_times = calculate_analysis_times(project_data)
        bottlenecks = identify_bottlenecks(project_data)
        
        # AdaugÄƒ informaÈ›ii despre structura de directoare dacÄƒ existÄƒ
        directory_info = ""
        if project_data.get('directory_structure'):
            directory_info = f"""
## ğŸ“ STRUCTURA DIRECTOARE
- Total directoare: {project_data['directory_structure'].get('directories', 0)}
- Total fiÈ™iere: {project_data['directory_structure'].get('total_files', 0)}
- FiÈ™iere Python: {project_data['directory_structure'].get('python_files', 0)}
- Puncte de intrare: {', '.join([ep['name'] for ep in project_data['directory_structure'].get('entry_points', [])])}
"""
        
        # Prompt pentru Claude
        prompt = f"""AnalizeazÄƒ urmÄƒtoarea structurÄƒ de proiect Python È™i genereazÄƒ un document workflow profesional.

## ğŸ“Š DATE PROIECT
```json
{json.dumps(project_data, indent=2)}
```

{directory_info}

## â±ï¸ TIMPI ESTIMAÈšI ANALIZÄ‚
- AnalizÄƒ scripturi: {analysis_times['script_analysis']}s
- AnalizÄƒ dependenÈ›e: {analysis_times['dependency_analysis']}s  
- Detectare pattern-uri: {analysis_times['pattern_analysis']}s
- Generare raport: {analysis_times['report_generation']}s
- **TOTAL: {analysis_times['total']}s**

## ğŸ”¥ PROBLEME IDENTIFICATE
{json.dumps(bottlenecks, indent=2)}

## ğŸ“‹ CERINÈšE PENTRU DOCUMENTUL GENERAT:

CreeazÄƒ un workflow vizual profesional care sÄƒ includÄƒ:

### ğŸš€ 1. PREZENTARE GENERALÄ‚ PROIECT
- Titlu cu emoji È™i numele proiectului
- Timeline total estimat
- Tipul aplicaÈ›iei detectat
- Structura de directoare (dacÄƒ existÄƒ)

### â¬‡ï¸ 2. FLUX PRINCIPAL ANALIZÄ‚ (cu timeline-uri È™i emoji-uri)

**ğŸ” PASUL 1: ANALIZÄ‚ SCRIPTURI** (~{analysis_times['script_analysis']}s)
- Input: Script principal + {len(project_data.get('secondary_scripts', []))} scripturi secundare
- Process: Parsare AST, extragere entitÄƒÈ›i
- Output: Structura detaliatÄƒ

**ğŸ”— PASUL 2: MAPARE DEPENDENÈšE** (~{analysis_times['dependency_analysis']}s)
- Input: EntitÄƒÈ›i + importuri
- Process: Detectare conexiuni
- Output: Graf dependenÈ›e + {len(project_data.get('connections', []))} conexiuni

**ğŸ¯ PASUL 3: DETECTARE PATTERN-URI** (~{analysis_times['pattern_analysis']}s)  
- Input: Structura + dependenÈ›e
- Process: AnalizÄƒ arhitecturalÄƒ
- Output: Pattern-uri + recomandÄƒri

**ğŸ“„ PASUL 4: GENERARE RAPORT** (~{analysis_times['report_generation']}s)
- Input: Toate analizele
- Process: Formatare + export
- Output: Raport final

### ğŸ”¥ 3. PROBLEME IDENTIFICATE
Lista problemelor cu severitate:
- **CRITICAL** ğŸ”´: Probleme majore
- **WARNING** ğŸŸ¡: AtenÈ›ionÄƒri
- **INFO** ğŸ”µ: ObservaÈ›ii

### ğŸ“Š 4. METRICI DE PERFORMANÈšÄ‚  
Tabel cu:
- Timp total analizÄƒ: {analysis_times['total']}s
- NumÄƒrul de fiÈ™iere: {project_data.get('metrics', {}).get('total_files', 'N/A')}
- NumÄƒrul de conexiuni: {len(project_data.get('connections', []))}
- Complexitate: {project_data.get('metrics', {}).get('coupling_score', 'medie')}
- Tip aplicaÈ›ie: {project_data.get('main_script', {}).get('analysis', {}).get('script_type', 'modul')}

### ğŸ—‚ï¸ 5. FIÈ˜IERE IMPLICATE ÃN WORKFLOW
Lista organizatÄƒ pe categorii:
- **SCRIPT PRINCIPAL**: {project_data.get('main_script', {}).get('name', 'N/A')}
- **SCRIPTURI SECUNDARE**: Lista tuturor scripturilor secundare
- **DEPENDENÈšE**: Top importuri utilizate

### ğŸ’¡ 6. RECOMANDÄ‚RI
- Sugestii de optimizare bazate pe analizÄƒ
- Best practices pentru arhitectura identificatÄƒ
- UrmÄƒtorii paÈ™i recomandaÈ›i

FoloseÈ™te markdown cu formatare profesionalÄƒ, emoji-uri relevante È™i structurÄƒ clarÄƒ.
NU menÈ›iona niciun tool extern sau platformÄƒ specificÄƒ. ConcentreazÄƒ-te pe analiza Python Forensics (p4n6)."""
        
        # FAZA 3.2 - Try-catch pentru apel API cu gestionare erori
        try:
            # Apel cÄƒtre Claude API
            response = requests.post(
                'https://api.anthropic.com/v1/messages',
                headers={
                    'x-api-key': api_key,
                    'anthropic-version': '2023-06-01',
                    'content-type': 'application/json'
                },
                json={
                    'model': 'claude-3-opus-20240229',
                    'max_tokens': 4000,
                    'messages': [{
                        'role': 'user',
                        'content': prompt
                    }]
                },
                timeout=30  # Timeout de 30 secunde
            )
            
            if response.status_code == 200:
                result = response.json()
                workflow_content = result['content'][0]['text']
                
                return jsonify({
                    'status': 'ok',
                    'workflow': workflow_content
                })
            elif response.status_code == 401:
                return jsonify({
                    'status': 'error',
                    'message': 'Cheia API este invalidÄƒ. VerificaÈ›i ANTHROPIC_API_KEY Ã®n fiÈ™ierul .env'
                }), 401
            elif response.status_code == 429:
                return jsonify({
                    'status': 'error',
                    'message': 'LimitÄƒ de rate atinsÄƒ. VÄƒ rugÄƒm sÄƒ Ã®ncercaÈ›i din nou Ã®n cÃ¢teva momente'
                }), 429
            else:
                return jsonify({
                    'status': 'error',
                    'message': f'Eroare API Claude: Status {response.status_code}'
                }), response.status_code
                
        except requests.exceptions.Timeout:
            return jsonify({
                'status': 'error',
                'message': 'Timpul de aÈ™teptare pentru API a expirat. VÄƒ rugÄƒm sÄƒ Ã®ncercaÈ›i din nou'
            }), 504
        except requests.exceptions.ConnectionError:
            return jsonify({
                'status': 'error',
                'message': 'Nu s-a putut conecta la API-ul Claude. VerificaÈ›i conexiunea la internet'
            }), 503
        except Exception as api_error:
            return jsonify({
                'status': 'error',
                'message': f'Eroare la comunicarea cu API: {str(api_error)}'
            }), 500
            
    except Exception as e:
        # FAZA 3.2 - Eroare generalÄƒ
        return jsonify({
            'status': 'error',
            'message': f'Eroare la generarea workflow: {str(e)}'
        }), 500

@app.route('/export_pdf', methods=['POST'])
def export_pdf():
    """ExporteazÄƒ workflow-ul ca PDF"""
    try:
        data = request.get_json()
        content = data.get('content', '')
        title = data.get('title', 'Python Forensics - AnalizÄƒ')
        
        # FAZA 3.2 - Mesaj user-friendly pentru funcÈ›ionalitate neimplementatÄƒ
        pdf_enabled = os.getenv('ENABLE_PDF_EXPORT', 'False').lower() == 'true'
        
        if not pdf_enabled:
            return jsonify({
                'status': 'error',
                'message': 'FuncÈ›ionalitatea de export PDF nu este activatÄƒ. Pentru a o activa, instalaÈ›i librÄƒria "reportlab" sau "weasyprint" È™i setaÈ›i ENABLE_PDF_EXPORT=True Ã®n fiÈ™ierul .env'
            }), 501
        
        # TODO: Implementare export PDF cÃ¢nd librÄƒria va fi instalatÄƒ
        return jsonify({
            'status': 'error',
            'message': 'FuncÈ›ionalitatea de export PDF este Ã®n dezvoltare. FolosiÈ›i Export Text pentru moment.'
        }), 501
        
    except Exception as e:
        # FAZA 3.2
        return jsonify({
            'status': 'error',
            'message': f'Eroare la export PDF: {str(e)}'
        }), 500

@app.route('/get_secundare', methods=['GET'])
def get_secundare():
    """ReturneazÄƒ lista scripturilor secundare"""
    try:
        base_dir = os.path.dirname(__file__)
        parent_dir = os.path.abspath(os.path.join(base_dir, ".."))
        secundare_txt_path = os.path.join(parent_dir, 'secundare.txt')
        
        if not os.path.isfile(secundare_txt_path):
            return "", 200
            
        with open(secundare_txt_path, 'r', encoding='utf-8') as f:
            continut = f.read()
            
        return continut, 200, {'Content-Type': 'text/plain; charset=utf-8'}
        
    except Exception as e:
        # FAZA 3.2
        return f"Eroare: {str(e)}", 500, {'Content-Type': 'text/plain; charset=utf-8'}

# FAZA 2.1 - Endpoint pentru curÄƒÈ›are manualÄƒ cache (util pentru administrare)
@app.route('/clear_session_cache', methods=['POST'])
def clear_session_cache():
    """CurÄƒÈ›Äƒ manual cache-ul sesiunii"""
    try:
        # VerificÄƒ dacÄƒ existÄƒ o cheie de administrare
        admin_key = request.headers.get('X-Admin-Key')
        expected_key = os.getenv('ADMIN_KEY')
        
        if expected_key and admin_key != expected_key:
            return jsonify({
                'status': 'error',
                'message': 'Cheie de administrare invalidÄƒ'
            }), 403
        
        old_size = session_edits.size()
        session_edits.clear()
        
        return jsonify({
            'status': 'ok',
            'message': f'Cache curÄƒÈ›at. {old_size} fiÈ™iere eliminate'
        })
        
    except Exception as e:
        return jsonify({
            'status': 'error',
            'message': f'Eroare la curÄƒÈ›area cache-ului: {str(e)}'
        }), 500

if __name__ == '__main__':
    # Configurare din environment
    host = os.getenv('HOST', '0.0.0.0')
    port = int(os.getenv('PORT', 5000))
    debug = os.getenv('FLASK_DEBUG', 'True').lower() == 'true'
    
    app.run(host=host, port=port, debug=debug)